{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ISIBrnoAIMT Encoder with Attention Decoder\n",
    "\n",
    "Encoder was taken from the winner of the [Will Two Do?](https://physionet.org/content/challenge-2021/1.0.3/sources/) challenge [ISIBrnoAIMT](https://www.cinc.org/archives/2021/pdf/CinC2021-014.pdf)\n",
    "Decoder was taken from the [sequence to sequence tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) from Pytorch."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a195e52a52decb2a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, jaccard_score, confusion_matrix, precision_score, recall_score, accuracy_score\n",
    "\n",
    "from models.m04_EcgToText_ISIBrnoAIMT.dataset import *\n",
    "from models.m04_EcgToText_ISIBrnoAIMT.model import *\n",
    "from models.m04_EcgToText_ISIBrnoAIMT.train import *"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-24T05:13:45.563018Z",
     "start_time": "2024-05-24T05:13:43.178666Z"
    }
   },
   "id": "initial_id",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-24T05:13:45.578220Z",
     "start_time": "2024-05-24T05:13:45.568992Z"
    }
   },
   "id": "5b78b990737513dc",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train with different Encoder hidden size"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9d10629ccf53312"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-24T07:15:28.023092Z",
     "start_time": "2024-05-24T07:15:28.014120Z"
    }
   },
   "id": "e90f1444505bb053",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################ encoder hidden size: 128 ############################\n",
      "encoder #parameters: 1650560\n",
      "decoder #parameters: 2055397\n",
      "0m 23s (- 19m 9s) (1 2.0%) | Train Loss: 0.5922 | Val METEOR: 0.3593\n",
      "0m 41s (- 16m 47s) (2 4.0%) | Train Loss: 0.2485 | Val METEOR: 0.3862\n",
      "1m 0s (- 15m 46s) (3 6.0%) | Train Loss: 0.2114 | Val METEOR: 0.4228\n",
      "1m 18s (- 15m 5s) (4 8.0%) | Train Loss: 0.1902 | Val METEOR: 0.3936\n",
      "1m 37s (- 14m 33s) (5 10.0%) | Train Loss: 0.1738 | Val METEOR: 0.4358\n",
      "1m 55s (- 14m 6s) (6 12.0%) | Train Loss: 0.1627 | Val METEOR: 0.4383\n",
      "2m 13s (- 13m 42s) (7 14.0%) | Train Loss: 0.1526 | Val METEOR: 0.4161\n",
      "2m 32s (- 13m 19s) (8 16.0%) | Train Loss: 0.1451 | Val METEOR: 0.4464\n",
      "2m 50s (- 12m 57s) (9 18.0%) | Train Loss: 0.1383 | Val METEOR: 0.447\n",
      "3m 9s (- 12m 36s) (10 20.0%) | Train Loss: 0.1333 | Val METEOR: 0.4418\n",
      "3m 27s (- 12m 15s) (11 22.0%) | Train Loss: 0.128 | Val METEOR: 0.457\n",
      "3m 45s (- 11m 54s) (12 24.0%) | Train Loss: 0.1229 | Val METEOR: 0.4685\n",
      "4m 4s (- 11m 34s) (13 26.0%) | Train Loss: 0.1191 | Val METEOR: 0.458\n",
      "4m 22s (- 11m 14s) (14 28.0%) | Train Loss: 0.1157 | Val METEOR: 0.4515\n",
      "4m 40s (- 10m 55s) (15 30.0%) | Train Loss: 0.1124 | Val METEOR: 0.4634\n",
      "4m 59s (- 10m 35s) (16 32.0%) | Train Loss: 0.1091 | Val METEOR: 0.4604\n",
      "5m 17s (- 10m 16s) (17 34.0%) | Train Loss: 0.1074 | Val METEOR: 0.4595\n",
      "5m 35s (- 9m 56s) (18 36.0%) | Train Loss: 0.1046 | Val METEOR: 0.4713\n",
      "5m 54s (- 9m 37s) (19 38.0%) | Train Loss: 0.102 | Val METEOR: 0.4775\n",
      "6m 12s (- 9m 18s) (20 40.0%) | Train Loss: 0.0993 | Val METEOR: 0.4623\n",
      "6m 30s (- 8m 59s) (21 42.0%) | Train Loss: 0.0967 | Val METEOR: 0.4875\n",
      "6m 49s (- 8m 40s) (22 44.0%) | Train Loss: 0.0956 | Val METEOR: 0.4818\n",
      "7m 7s (- 8m 21s) (23 46.0%) | Train Loss: 0.0928 | Val METEOR: 0.4824\n",
      "7m 25s (- 8m 3s) (24 48.0%) | Train Loss: 0.0914 | Val METEOR: 0.4775\n",
      "7m 44s (- 7m 44s) (25 50.0%) | Train Loss: 0.0892 | Val METEOR: 0.4667\n",
      "8m 2s (- 7m 25s) (26 52.0%) | Train Loss: 0.0869 | Val METEOR: 0.483\n",
      "8m 21s (- 7m 6s) (27 54.0%) | Train Loss: 0.0856 | Val METEOR: 0.4785\n",
      "8m 39s (- 6m 48s) (28 56.0%) | Train Loss: 0.0838 | Val METEOR: 0.4914\n",
      "8m 58s (- 6m 29s) (29 58.0%) | Train Loss: 0.0829 | Val METEOR: 0.4891\n",
      "9m 16s (- 6m 10s) (30 60.0%) | Train Loss: 0.0815 | Val METEOR: 0.4466\n",
      "9m 34s (- 5m 52s) (31 62.0%) | Train Loss: 0.0792 | Val METEOR: 0.5032\n",
      "9m 53s (- 5m 33s) (32 64.0%) | Train Loss: 0.079 | Val METEOR: 0.4837\n",
      "10m 11s (- 5m 15s) (33 66.0%) | Train Loss: 0.0777 | Val METEOR: 0.495\n",
      "10m 30s (- 4m 56s) (34 68.0%) | Train Loss: 0.0759 | Val METEOR: 0.493\n",
      "10m 48s (- 4m 37s) (35 70.0%) | Train Loss: 0.0744 | Val METEOR: 0.4646\n",
      "11m 7s (- 4m 19s) (36 72.0%) | Train Loss: 0.0733 | Val METEOR: 0.4898\n",
      "11m 25s (- 4m 0s) (37 74.0%) | Train Loss: 0.0724 | Val METEOR: 0.4857\n",
      "11m 43s (- 3m 42s) (38 76.0%) | Train Loss: 0.0719 | Val METEOR: 0.4888\n",
      "12m 2s (- 3m 23s) (39 78.0%) | Train Loss: 0.0703 | Val METEOR: 0.4911\n",
      "12m 20s (- 3m 5s) (40 80.0%) | Train Loss: 0.0702 | Val METEOR: 0.4741\n",
      "12m 39s (- 2m 46s) (41 82.0%) | Train Loss: 0.0687 | Val METEOR: 0.5033\n",
      "12m 57s (- 2m 28s) (42 84.0%) | Train Loss: 0.0676 | Val METEOR: 0.4921\n",
      "13m 15s (- 2m 9s) (43 86.0%) | Train Loss: 0.0665 | Val METEOR: 0.4868\n",
      "13m 34s (- 1m 51s) (44 88.0%) | Train Loss: 0.0653 | Val METEOR: 0.4239\n",
      "13m 52s (- 1m 32s) (45 90.0%) | Train Loss: 0.0643 | Val METEOR: 0.4948\n",
      "14m 11s (- 1m 14s) (46 92.0%) | Train Loss: 0.0635 | Val METEOR: 0.4835\n",
      "14m 29s (- 0m 55s) (47 94.0%) | Train Loss: 0.0625 | Val METEOR: 0.5009\n",
      "14m 47s (- 0m 36s) (48 96.0%) | Train Loss: 0.0609 | Val METEOR: 0.4838\n",
      "15m 6s (- 0m 18s) (49 98.0%) | Train Loss: 0.0624 | Val METEOR: 0.4535\n",
      "15m 24s (- 0m 0s) (50 100.0%) | Train Loss: 0.0613 | Val METEOR: 0.4916\n",
      "############################ encoder hidden size: 256 ############################\n",
      "encoder #parameters: 6545152\n",
      "decoder #parameters: 2219237\n",
      "0m 27s (- 22m 16s) (1 2.0%) | Train Loss: 0.5712 | Val METEOR: 0.3314\n",
      "0m 54s (- 21m 48s) (2 4.0%) | Train Loss: 0.2462 | Val METEOR: 0.3629\n",
      "1m 21s (- 21m 21s) (3 6.0%) | Train Loss: 0.2109 | Val METEOR: 0.3774\n",
      "1m 48s (- 20m 52s) (4 8.0%) | Train Loss: 0.1897 | Val METEOR: 0.382\n",
      "2m 16s (- 20m 24s) (5 10.0%) | Train Loss: 0.1747 | Val METEOR: 0.3872\n",
      "2m 43s (- 19m 57s) (6 12.0%) | Train Loss: 0.1636 | Val METEOR: 0.43\n",
      "3m 10s (- 19m 30s) (7 14.0%) | Train Loss: 0.155 | Val METEOR: 0.4066\n",
      "3m 37s (- 19m 2s) (8 16.0%) | Train Loss: 0.1457 | Val METEOR: 0.4299\n",
      "4m 4s (- 18m 34s) (9 18.0%) | Train Loss: 0.1398 | Val METEOR: 0.4421\n",
      "4m 31s (- 18m 7s) (10 20.0%) | Train Loss: 0.1345 | Val METEOR: 0.4252\n",
      "4m 58s (- 17m 40s) (11 22.0%) | Train Loss: 0.1297 | Val METEOR: 0.4328\n",
      "5m 26s (- 17m 12s) (12 24.0%) | Train Loss: 0.1251 | Val METEOR: 0.4248\n",
      "5m 53s (- 16m 45s) (13 26.0%) | Train Loss: 0.1212 | Val METEOR: 0.4505\n",
      "6m 20s (- 16m 18s) (14 28.0%) | Train Loss: 0.118 | Val METEOR: 0.4612\n",
      "6m 47s (- 15m 51s) (15 30.0%) | Train Loss: 0.1148 | Val METEOR: 0.457\n",
      "7m 14s (- 15m 23s) (16 32.0%) | Train Loss: 0.1119 | Val METEOR: 0.3929\n",
      "7m 42s (- 14m 56s) (17 34.0%) | Train Loss: 0.1085 | Val METEOR: 0.4444\n",
      "8m 9s (- 14m 29s) (18 36.0%) | Train Loss: 0.1063 | Val METEOR: 0.4708\n",
      "8m 36s (- 14m 2s) (19 38.0%) | Train Loss: 0.104 | Val METEOR: 0.4808\n",
      "9m 3s (- 13m 35s) (20 40.0%) | Train Loss: 0.1013 | Val METEOR: 0.4448\n",
      "9m 30s (- 13m 8s) (21 42.0%) | Train Loss: 0.0991 | Val METEOR: 0.4765\n",
      "9m 57s (- 12m 40s) (22 44.0%) | Train Loss: 0.0965 | Val METEOR: 0.4764\n",
      "10m 25s (- 12m 13s) (23 46.0%) | Train Loss: 0.095 | Val METEOR: 0.4798\n",
      "10m 52s (- 11m 46s) (24 48.0%) | Train Loss: 0.0938 | Val METEOR: 0.4815\n",
      "11m 19s (- 11m 19s) (25 50.0%) | Train Loss: 0.0942 | Val METEOR: 0.4736\n",
      "11m 46s (- 10m 52s) (26 52.0%) | Train Loss: 0.0908 | Val METEOR: 0.474\n",
      "12m 13s (- 10m 24s) (27 54.0%) | Train Loss: 0.0878 | Val METEOR: 0.4757\n",
      "12m 40s (- 9m 57s) (28 56.0%) | Train Loss: 0.0863 | Val METEOR: 0.4744\n",
      "13m 7s (- 9m 30s) (29 58.0%) | Train Loss: 0.085 | Val METEOR: 0.4803\n",
      "13m 35s (- 9m 3s) (30 60.0%) | Train Loss: 0.0839 | Val METEOR: 0.4883\n",
      "14m 2s (- 8m 36s) (31 62.0%) | Train Loss: 0.082 | Val METEOR: 0.4993\n",
      "14m 29s (- 8m 9s) (32 64.0%) | Train Loss: 0.0807 | Val METEOR: 0.4524\n",
      "14m 56s (- 7m 41s) (33 66.0%) | Train Loss: 0.0798 | Val METEOR: 0.4727\n",
      "15m 23s (- 7m 14s) (34 68.0%) | Train Loss: 0.0786 | Val METEOR: 0.4757\n",
      "15m 51s (- 6m 47s) (35 70.0%) | Train Loss: 0.0787 | Val METEOR: 0.4879\n",
      "16m 18s (- 6m 20s) (36 72.0%) | Train Loss: 0.0755 | Val METEOR: 0.4716\n",
      "16m 45s (- 5m 53s) (37 74.0%) | Train Loss: 0.0745 | Val METEOR: 0.4645\n",
      "17m 12s (- 5m 26s) (38 76.0%) | Train Loss: 0.0737 | Val METEOR: 0.4906\n",
      "17m 39s (- 4m 58s) (39 78.0%) | Train Loss: 0.0722 | Val METEOR: 0.4927\n",
      "18m 6s (- 4m 31s) (40 80.0%) | Train Loss: 0.0716 | Val METEOR: 0.4802\n",
      "18m 33s (- 4m 4s) (41 82.0%) | Train Loss: 0.0705 | Val METEOR: 0.4693\n",
      "No improvement in bleu score for 10 consecutive epochs. Stopping early.\n",
      "############################ encoder hidden size: 512 ############################\n",
      "encoder #parameters: 26066432\n",
      "decoder #parameters: 2546917\n",
      "0m 49s (- 40m 25s) (1 2.0%) | Train Loss: 0.6075 | Val METEOR: 0.3403\n",
      "1m 39s (- 39m 36s) (2 4.0%) | Train Loss: 0.2542 | Val METEOR: 0.3539\n",
      "2m 28s (- 38m 50s) (3 6.0%) | Train Loss: 0.2189 | Val METEOR: 0.3933\n",
      "3m 18s (- 38m 4s) (4 8.0%) | Train Loss: 0.1995 | Val METEOR: 0.3296\n",
      "4m 7s (- 37m 5s) (5 10.0%) | Train Loss: 1.5166 | Val METEOR: 0.0\n",
      "4m 55s (- 36m 9s) (6 12.0%) | Train Loss: 1.6632 | Val METEOR: 0.0\n",
      "5m 44s (- 35m 17s) (7 14.0%) | Train Loss: 1.6618 | Val METEOR: 0.0\n",
      "6m 33s (- 34m 24s) (8 16.0%) | Train Loss: 1.6611 | Val METEOR: 0.0\n",
      "7m 21s (- 33m 33s) (9 18.0%) | Train Loss: 1.6996 | Val METEOR: 0.0\n",
      "8m 10s (- 32m 41s) (10 20.0%) | Train Loss: 1.7102 | Val METEOR: 0.0\n",
      "8m 58s (- 31m 50s) (11 22.0%) | Train Loss: 1.6597 | Val METEOR: 0.0\n",
      "9m 47s (- 31m 0s) (12 24.0%) | Train Loss: 1.6616 | Val METEOR: 0.0\n",
      "10m 35s (- 30m 9s) (13 26.0%) | Train Loss: 1.6622 | Val METEOR: 0.0\n",
      "No improvement in bleu score for 10 consecutive epochs. Stopping early.\n",
      "############################ encoder hidden size: 1024 ############################\n",
      "encoder #parameters: 104037376\n",
      "decoder #parameters: 3202277\n",
      "1m 51s (- 90m 56s) (1 2.0%) | Train Loss: 1.7246 | Val METEOR: 0.0\n",
      "3m 40s (- 88m 22s) (2 4.0%) | Train Loss: 1.6613 | Val METEOR: 0.0\n",
      "5m 30s (- 86m 11s) (3 6.0%) | Train Loss: 1.6592 | Val METEOR: 0.0\n",
      "7m 19s (- 84m 10s) (4 8.0%) | Train Loss: 1.6558 | Val METEOR: 0.0\n",
      "9m 7s (- 82m 11s) (5 10.0%) | Train Loss: 1.6556 | Val METEOR: 0.0\n",
      "10m 56s (- 80m 15s) (6 12.0%) | Train Loss: 1.6544 | Val METEOR: 0.0\n",
      "12m 45s (- 78m 22s) (7 14.0%) | Train Loss: 1.6575 | Val METEOR: 0.0\n",
      "14m 34s (- 76m 30s) (8 16.0%) | Train Loss: 1.6584 | Val METEOR: 0.0\n",
      "16m 23s (- 74m 39s) (9 18.0%) | Train Loss: 1.6574 | Val METEOR: 0.0\n",
      "18m 12s (- 72m 49s) (10 20.0%) | Train Loss: 1.6556 | Val METEOR: 0.0\n",
      "No improvement in bleu score for 10 consecutive epochs. Stopping early.\n",
      "############################ encoder hidden size: 2048 ############################\n",
      "encoder #parameters: 415692800\n",
      "decoder #parameters: 4512997\n",
      "24m 48s (- 1215m 13s) (1 2.0%) | Train Loss: 1.7337 | Val METEOR: 0.0\n",
      "49m 35s (- 1190m 5s) (2 4.0%) | Train Loss: 1.6599 | Val METEOR: 0.0\n",
      "74m 22s (- 1165m 11s) (3 6.0%) | Train Loss: 1.6689 | Val METEOR: 0.0\n",
      "99m 9s (- 1140m 21s) (4 8.0%) | Train Loss: 1.6699 | Val METEOR: 0.0\n",
      "123m 56s (- 1115m 31s) (5 10.0%) | Train Loss: 1.6602 | Val METEOR: 0.0\n",
      "148m 43s (- 1090m 41s) (6 12.0%) | Train Loss: 1.6606 | Val METEOR: 0.0\n",
      "173m 30s (- 1065m 52s) (7 14.0%) | Train Loss: 1.6604 | Val METEOR: 0.0\n",
      "198m 17s (- 1041m 3s) (8 16.0%) | Train Loss: 1.659 | Val METEOR: 0.0\n",
      "231m 39s (- 1055m 20s) (9 18.0%) | Train Loss: 1.6596 | Val METEOR: 0.0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 25\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoder #parameters: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcount_parameters(encoder)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdecoder #parameters: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcount_parameters(decoder)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 25\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecoder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlanguage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_size\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\ecg-to-text\\models\\m04_EcgToText_ISIBrnoAIMT\\train.py:166\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(train_dataloader, val_dataloader, encoder, decoder, criterion, output_lang, n_epochs, learning_rate, patience, max_grad_norm, size)\u001B[0m\n\u001B[0;32m    163\u001B[0m early_stopping_counter \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, n_epochs \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m--> 166\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecoder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoder_optimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecoder_optimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_grad_norm\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    167\u001B[0m     val_loss, f1, jaccard, rouge, meteor \u001B[38;5;241m=\u001B[39m validate_epoch(val_dataloader, encoder, decoder, criterion, output_lang)\n\u001B[0;32m    170\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtimeSince(start,\u001B[38;5;250m \u001B[39mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;250m \u001B[39mn_epochs)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mround\u001B[39m(epoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;250m \u001B[39mn_epochs\u001B[38;5;250m \u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m100\u001B[39m,\u001B[38;5;250m \u001B[39m\u001B[38;5;241m2\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m%) | \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    171\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTrain Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mround\u001B[39m(train_loss,\u001B[38;5;250m \u001B[39m\u001B[38;5;241m4\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m | Val METEOR: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mround\u001B[39m(meteor,\u001B[38;5;250m \u001B[39m\u001B[38;5;241m4\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\ecg-to-text\\models\\m04_EcgToText_ISIBrnoAIMT\\train.py:48\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[1;34m(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_norm)\u001B[0m\n\u001B[0;32m     44\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     46\u001B[0m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mclip_grad_norm_(\u001B[38;5;28mlist\u001B[39m(encoder\u001B[38;5;241m.\u001B[39mparameters()) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mlist\u001B[39m(decoder\u001B[38;5;241m.\u001B[39mparameters()), max_norm)\n\u001B[1;32m---> 48\u001B[0m \u001B[43mencoder_optimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     49\u001B[0m decoder_optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     51\u001B[0m total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\PycharmProjects\\ecg-to-text\\venv\\lib\\site-packages\\torch\\optim\\optimizer.py:385\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    380\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    381\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    382\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    383\u001B[0m             )\n\u001B[1;32m--> 385\u001B[0m out \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    386\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[0;32m    388\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\ecg-to-text\\venv\\lib\\site-packages\\torch\\optim\\optimizer.py:76\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     74\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m     75\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[1;32m---> 76\u001B[0m     ret \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     78\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[1;32m~\\PycharmProjects\\ecg-to-text\\venv\\lib\\site-packages\\torch\\optim\\adam.py:166\u001B[0m, in \u001B[0;36mAdam.step\u001B[1;34m(self, closure)\u001B[0m\n\u001B[0;32m    155\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m    157\u001B[0m     has_complex \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[0;32m    158\u001B[0m         group,\n\u001B[0;32m    159\u001B[0m         params_with_grad,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    163\u001B[0m         max_exp_avg_sqs,\n\u001B[0;32m    164\u001B[0m         state_steps)\n\u001B[1;32m--> 166\u001B[0m     \u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    167\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    168\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    169\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    170\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    171\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    172\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    173\u001B[0m \u001B[43m        \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mamsgrad\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    174\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    176\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    177\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    178\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    179\u001B[0m \u001B[43m        \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    180\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    181\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforeach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mforeach\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    182\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcapturable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    183\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdifferentiable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    184\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfused\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfused\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    185\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgrad_scale\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    186\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfound_inf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    187\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    189\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[1;32m~\\PycharmProjects\\ecg-to-text\\venv\\lib\\site-packages\\torch\\optim\\adam.py:316\u001B[0m, in \u001B[0;36madam\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[0;32m    313\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    314\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adam\n\u001B[1;32m--> 316\u001B[0m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    317\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    318\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    319\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    320\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    321\u001B[0m \u001B[43m     \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    322\u001B[0m \u001B[43m     \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    323\u001B[0m \u001B[43m     \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    324\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    325\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    326\u001B[0m \u001B[43m     \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    327\u001B[0m \u001B[43m     \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    328\u001B[0m \u001B[43m     \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    329\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    330\u001B[0m \u001B[43m     \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    331\u001B[0m \u001B[43m     \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    332\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    333\u001B[0m \u001B[43m     \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfound_inf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\ecg-to-text\\venv\\lib\\site-packages\\torch\\optim\\adam.py:583\u001B[0m, in \u001B[0;36m_multi_tensor_adam\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001B[0m\n\u001B[0;32m    581\u001B[0m torch\u001B[38;5;241m.\u001B[39m_foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001B[0;32m    582\u001B[0m torch\u001B[38;5;241m.\u001B[39m_foreach_add_(exp_avg_sq_sqrt, eps)\n\u001B[1;32m--> 583\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_foreach_addcdiv_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_exp_avgs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexp_avg_sq_sqrt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstep_size\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "language, dataloader = get_dataloader(file_path='./data_ptb-xl', batch_size=64, mode='train', device=device)\n",
    "_, val_dataloader = get_dataloader(file_path='./data_ptb-xl', batch_size=64, mode='val', device=device, _lang=language)\n",
    "    \n",
    "n_epochs=50\n",
    "encoder_hidden = [128, 256, 512, 1024, 2048]\n",
    "decoder_hidden_size = 256\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "for encoder_hidden_size in encoder_hidden:\n",
    "    encoder = NN(num_leads=12,\n",
    "                 hidden_size=encoder_hidden_size).to(device)\n",
    "    decoder = AttnDecoderRNN(hidden_size=decoder_hidden_size,\n",
    "                             encoder_hidden_size=encoder_hidden_size,\n",
    "                             output_size=language.n_words,\n",
    "                             max_len=language.max_len).to(device)\n",
    "    \n",
    "    print(f\"############################ encoder hidden size: {encoder_hidden_size} ############################\")\n",
    "    print(f\"encoder #parameters: {count_parameters(encoder)}\")\n",
    "    print(f\"decoder #parameters: {count_parameters(decoder)}\\n\")\n",
    "\n",
    "    train(dataloader, val_dataloader, encoder, decoder, criterion, language, n_epochs, size=encoder_hidden_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-23T21:53:34.767067Z",
     "start_time": "2024-05-23T16:25:20.330012Z"
    }
   },
   "id": "7b8bfda8657afe70",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13d46327f50a1c27"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "       Encoder  Encoder #parameters  Decoder #parameters  Test Loss  \\\n0  Encoder 128              1650560              2055397     3.6968   \n1  Encoder 256              6545152              2219237     3.4800   \n2  Encoder 512             26066432              2546917     2.6557   \n\n   Rouge-1 (p)  Rouge-1 (r)  Rouge-1 (f1)  Rouge-2 (p)  Rouge-2 (r)  \\\n0        0.581        0.600         0.566        0.440        0.451   \n1        0.587        0.606         0.572        0.447        0.459   \n2        0.507        0.502         0.476        0.346        0.324   \n\n   Rouge-2 (f1)  Rouge-L (p)  Rouge-L (r)  Rouge-L (f1)  METEOR  \n0         0.425        0.578        0.596         0.562   0.495  \n1         0.433        0.583        0.601         0.568   0.501  \n2         0.313        0.502        0.498         0.472   0.391  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Encoder</th>\n      <th>Encoder #parameters</th>\n      <th>Decoder #parameters</th>\n      <th>Test Loss</th>\n      <th>Rouge-1 (p)</th>\n      <th>Rouge-1 (r)</th>\n      <th>Rouge-1 (f1)</th>\n      <th>Rouge-2 (p)</th>\n      <th>Rouge-2 (r)</th>\n      <th>Rouge-2 (f1)</th>\n      <th>Rouge-L (p)</th>\n      <th>Rouge-L (r)</th>\n      <th>Rouge-L (f1)</th>\n      <th>METEOR</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Encoder 128</td>\n      <td>1650560</td>\n      <td>2055397</td>\n      <td>3.6968</td>\n      <td>0.581</td>\n      <td>0.600</td>\n      <td>0.566</td>\n      <td>0.440</td>\n      <td>0.451</td>\n      <td>0.425</td>\n      <td>0.578</td>\n      <td>0.596</td>\n      <td>0.562</td>\n      <td>0.495</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Encoder 256</td>\n      <td>6545152</td>\n      <td>2219237</td>\n      <td>3.4800</td>\n      <td>0.587</td>\n      <td>0.606</td>\n      <td>0.572</td>\n      <td>0.447</td>\n      <td>0.459</td>\n      <td>0.433</td>\n      <td>0.583</td>\n      <td>0.601</td>\n      <td>0.568</td>\n      <td>0.501</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Encoder 512</td>\n      <td>26066432</td>\n      <td>2546917</td>\n      <td>2.6557</td>\n      <td>0.507</td>\n      <td>0.502</td>\n      <td>0.476</td>\n      <td>0.346</td>\n      <td>0.324</td>\n      <td>0.313</td>\n      <td>0.502</td>\n      <td>0.498</td>\n      <td>0.472</td>\n      <td>0.391</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder_hidden = [128, 256, 512, 1024, 2048]\n",
    "decoder_hidden_size = 256\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "language, dataloader = get_dataloader(file_path='./data_ptb-xl', batch_size=64, mode='train', device=device)\n",
    "_, test_dataloader = get_dataloader(file_path='./data_ptb-xl', batch_size=64, mode='test', device=device, _lang=language)\n",
    "results = []\n",
    "\n",
    "# for encoder_hidden_size in encoder_hidden:\n",
    "for encoder_hidden_size in [128, 256, 512]:\n",
    "    encoder = NN(num_leads=12,\n",
    "                 hidden_size=encoder_hidden_size).to(device)\n",
    "    decoder = AttnDecoderRNN(hidden_size=decoder_hidden_size,\n",
    "                             encoder_hidden_size=encoder_hidden_size,\n",
    "                             output_size=language.n_words,\n",
    "                             max_len=language.max_len).to(device)\n",
    "    \n",
    "    encoder.load_state_dict(torch.load(f'./models/m04_EcgToText_ISIBrnoAIMT/models_with_different_hidden_size/varying_encoder/Encoder_{encoder_hidden_size}.pth'))\n",
    "    decoder.load_state_dict(torch.load(f'./models/m04_EcgToText_ISIBrnoAIMT/models_with_different_hidden_size/varying_encoder/Decoder_{encoder_hidden_size}.pth'))\n",
    "    \n",
    "    total_loss, f1, jaccard, rouge, meteor = validate_epoch(test_dataloader, encoder, decoder, criterion, language)\n",
    "\n",
    "    results.append({\n",
    "        \"Encoder\": f\"Encoder {encoder_hidden_size}\",\n",
    "        \"Encoder #parameters\": count_parameters(encoder),\n",
    "        \"Decoder #parameters\": count_parameters(decoder),\n",
    "        \"Test Loss\": round(total_loss, 4),\n",
    "        \"Rouge-1 (p)\": round(rouge[\"rouge-1\"][\"p\"], 3),\n",
    "        \"Rouge-1 (r)\": round(rouge[\"rouge-1\"][\"r\"], 3),\n",
    "        \"Rouge-1 (f1)\": round(rouge[\"rouge-1\"][\"f\"], 3),\n",
    "        \"Rouge-2 (p)\": round(rouge[\"rouge-2\"][\"p\"], 3),\n",
    "        \"Rouge-2 (r)\": round(rouge[\"rouge-2\"][\"r\"], 3),\n",
    "        \"Rouge-2 (f1)\": round(rouge[\"rouge-2\"][\"f\"], 3),\n",
    "        \"Rouge-L (p)\": round(rouge[\"rouge-l\"][\"p\"], 3),\n",
    "        \"Rouge-L (r)\": round(rouge[\"rouge-l\"][\"r\"], 3),\n",
    "        \"Rouge-L (f1)\": round(rouge[\"rouge-l\"][\"f\"], 3),\n",
    "        \"METEOR\": round(meteor, 3)\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-24T07:17:35.984262Z",
     "start_time": "2024-05-24T07:15:37.397623Z"
    }
   },
   "id": "ed2cc96a1ba41a5",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train with different Decoder hidden size"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "142744930a6e24b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-23T05:38:57.053394Z",
     "start_time": "2024-05-23T05:38:57.038578Z"
    }
   },
   "id": "fa00eff434c3fa84",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################ decoder hidden size: 128 ############################\n",
      "encoder #parameters: 6545152\n",
      "decoder #parameters: 996325\n",
      "0m 25s (- 20m 53s) (1 2.0%) | Train Loss: 0.9623 | Val METEOR: 0.3209\n",
      "0m 51s (- 20m 30s) (2 4.0%) | Train Loss: 0.3397 | Val METEOR: 0.3363\n",
      "1m 16s (- 20m 4s) (3 6.0%) | Train Loss: 0.2738 | Val METEOR: 0.3717\n",
      "1m 42s (- 19m 40s) (4 8.0%) | Train Loss: 0.2451 | Val METEOR: 0.3977\n",
      "2m 8s (- 19m 16s) (5 10.0%) | Train Loss: 0.2251 | Val METEOR: 0.3543\n",
      "2m 34s (- 18m 51s) (6 12.0%) | Train Loss: 0.2109 | Val METEOR: 0.4367\n",
      "3m 0s (- 18m 26s) (7 14.0%) | Train Loss: 0.1999 | Val METEOR: 0.3968\n",
      "3m 25s (- 18m 1s) (8 16.0%) | Train Loss: 0.1911 | Val METEOR: 0.4092\n",
      "3m 51s (- 17m 35s) (9 18.0%) | Train Loss: 0.183 | Val METEOR: 0.4392\n",
      "4m 17s (- 17m 11s) (10 20.0%) | Train Loss: 0.177 | Val METEOR: 0.4325\n",
      "4m 43s (- 16m 45s) (11 22.0%) | Train Loss: 0.1715 | Val METEOR: 0.4386\n",
      "5m 9s (- 16m 19s) (12 24.0%) | Train Loss: 0.1668 | Val METEOR: 0.4308\n",
      "5m 35s (- 15m 54s) (13 26.0%) | Train Loss: 0.1612 | Val METEOR: 0.4527\n",
      "6m 1s (- 15m 28s) (14 28.0%) | Train Loss: 0.1579 | Val METEOR: 0.4714\n",
      "6m 27s (- 15m 3s) (15 30.0%) | Train Loss: 0.1534 | Val METEOR: 0.4421\n",
      "6m 52s (- 14m 37s) (16 32.0%) | Train Loss: 0.1502 | Val METEOR: 0.4708\n",
      "7m 18s (- 14m 11s) (17 34.0%) | Train Loss: 0.1462 | Val METEOR: 0.4656\n",
      "7m 44s (- 13m 46s) (18 36.0%) | Train Loss: 0.1436 | Val METEOR: 0.4453\n",
      "8m 10s (- 13m 20s) (19 38.0%) | Train Loss: 0.1401 | Val METEOR: 0.4822\n",
      "8m 36s (- 12m 54s) (20 40.0%) | Train Loss: 0.1378 | Val METEOR: 0.4747\n",
      "9m 2s (- 12m 29s) (21 42.0%) | Train Loss: 0.1358 | Val METEOR: 0.4544\n",
      "9m 28s (- 12m 3s) (22 44.0%) | Train Loss: 0.1335 | Val METEOR: 0.4452\n",
      "9m 54s (- 11m 37s) (23 46.0%) | Train Loss: 0.1305 | Val METEOR: 0.4775\n",
      "10m 20s (- 11m 11s) (24 48.0%) | Train Loss: 0.1286 | Val METEOR: 0.4817\n",
      "10m 45s (- 10m 45s) (25 50.0%) | Train Loss: 0.1263 | Val METEOR: 0.4887\n",
      "11m 11s (- 10m 20s) (26 52.0%) | Train Loss: 0.1248 | Val METEOR: 0.4782\n",
      "11m 37s (- 9m 54s) (27 54.0%) | Train Loss: 0.1225 | Val METEOR: 0.4469\n",
      "12m 3s (- 9m 28s) (28 56.0%) | Train Loss: 0.12 | Val METEOR: 0.467\n",
      "12m 29s (- 9m 2s) (29 58.0%) | Train Loss: 0.1179 | Val METEOR: 0.4745\n",
      "12m 54s (- 8m 36s) (30 60.0%) | Train Loss: 0.1166 | Val METEOR: 0.4716\n",
      "13m 20s (- 8m 10s) (31 62.0%) | Train Loss: 0.1149 | Val METEOR: 0.4828\n",
      "13m 46s (- 7m 44s) (32 64.0%) | Train Loss: 0.1131 | Val METEOR: 0.4882\n",
      "14m 12s (- 7m 19s) (33 66.0%) | Train Loss: 0.1111 | Val METEOR: 0.5068\n",
      "14m 38s (- 6m 53s) (34 68.0%) | Train Loss: 0.11 | Val METEOR: 0.4997\n",
      "15m 4s (- 6m 27s) (35 70.0%) | Train Loss: 0.1074 | Val METEOR: 0.485\n",
      "15m 29s (- 6m 1s) (36 72.0%) | Train Loss: 0.1059 | Val METEOR: 0.4988\n",
      "15m 55s (- 5m 35s) (37 74.0%) | Train Loss: 0.1045 | Val METEOR: 0.4999\n",
      "16m 21s (- 5m 9s) (38 76.0%) | Train Loss: 0.1031 | Val METEOR: 0.4879\n",
      "16m 47s (- 4m 44s) (39 78.0%) | Train Loss: 0.1028 | Val METEOR: 0.4857\n",
      "17m 12s (- 4m 18s) (40 80.0%) | Train Loss: 0.1033 | Val METEOR: 0.4735\n",
      "17m 38s (- 3m 52s) (41 82.0%) | Train Loss: 0.0995 | Val METEOR: 0.4809\n",
      "18m 4s (- 3m 26s) (42 84.0%) | Train Loss: 0.0978 | Val METEOR: 0.5036\n",
      "18m 30s (- 3m 0s) (43 86.0%) | Train Loss: 0.0956 | Val METEOR: 0.4818\n",
      "No improvement in bleu score for 10 consecutive epochs. Stopping early.\n",
      "############################ decoder hidden size: 256 ############################\n",
      "encoder #parameters: 6545152\n",
      "decoder #parameters: 2219237\n",
      "0m 27s (- 22m 4s) (1 2.0%) | Train Loss: 0.7699 | Val METEOR: 0.3474\n",
      "0m 54s (- 21m 41s) (2 4.0%) | Train Loss: 0.2919 | Val METEOR: 0.3923\n",
      "1m 21s (- 21m 14s) (3 6.0%) | Train Loss: 0.2367 | Val METEOR: 0.3886\n",
      "1m 48s (- 20m 47s) (4 8.0%) | Train Loss: 0.209 | Val METEOR: 0.4273\n",
      "2m 15s (- 20m 21s) (5 10.0%) | Train Loss: 0.1907 | Val METEOR: 0.4197\n",
      "2m 42s (- 19m 54s) (6 12.0%) | Train Loss: 0.1766 | Val METEOR: 0.3763\n",
      "3m 9s (- 19m 27s) (7 14.0%) | Train Loss: 0.165 | Val METEOR: 0.4006\n",
      "3m 37s (- 19m 0s) (8 16.0%) | Train Loss: 0.1559 | Val METEOR: 0.4401\n",
      "4m 4s (- 18m 32s) (9 18.0%) | Train Loss: 0.149 | Val METEOR: 0.3942\n",
      "4m 31s (- 18m 5s) (10 20.0%) | Train Loss: 0.143 | Val METEOR: 0.4447\n",
      "4m 58s (- 17m 39s) (11 22.0%) | Train Loss: 0.1366 | Val METEOR: 0.4595\n",
      "5m 25s (- 17m 12s) (12 24.0%) | Train Loss: 0.1321 | Val METEOR: 0.4556\n",
      "5m 53s (- 16m 45s) (13 26.0%) | Train Loss: 0.1274 | Val METEOR: 0.3191\n",
      "6m 20s (- 16m 17s) (14 28.0%) | Train Loss: 0.1244 | Val METEOR: 0.4583\n",
      "6m 47s (- 15m 50s) (15 30.0%) | Train Loss: 0.1195 | Val METEOR: 0.4235\n",
      "7m 14s (- 15m 23s) (16 32.0%) | Train Loss: 0.1162 | Val METEOR: 0.4113\n",
      "7m 41s (- 14m 56s) (17 34.0%) | Train Loss: 0.1131 | Val METEOR: 0.4522\n",
      "8m 8s (- 14m 29s) (18 36.0%) | Train Loss: 0.11 | Val METEOR: 0.4501\n",
      "8m 35s (- 14m 1s) (19 38.0%) | Train Loss: 0.1068 | Val METEOR: 0.4509\n",
      "9m 3s (- 13m 34s) (20 40.0%) | Train Loss: 0.1041 | Val METEOR: 0.4612\n",
      "9m 30s (- 13m 7s) (21 42.0%) | Train Loss: 0.102 | Val METEOR: 0.4688\n",
      "9m 57s (- 12m 40s) (22 44.0%) | Train Loss: 0.0997 | Val METEOR: 0.4458\n",
      "10m 24s (- 12m 13s) (23 46.0%) | Train Loss: 0.0977 | Val METEOR: 0.4693\n",
      "10m 51s (- 11m 46s) (24 48.0%) | Train Loss: 0.0968 | Val METEOR: 0.4697\n",
      "11m 19s (- 11m 19s) (25 50.0%) | Train Loss: 0.0932 | Val METEOR: 0.4304\n",
      "11m 46s (- 10m 51s) (26 52.0%) | Train Loss: 0.0915 | Val METEOR: 0.4723\n",
      "12m 13s (- 10m 24s) (27 54.0%) | Train Loss: 0.0901 | Val METEOR: 0.4724\n",
      "12m 40s (- 9m 57s) (28 56.0%) | Train Loss: 0.089 | Val METEOR: 0.4755\n",
      "13m 7s (- 9m 30s) (29 58.0%) | Train Loss: 0.0879 | Val METEOR: 0.4746\n",
      "13m 35s (- 9m 3s) (30 60.0%) | Train Loss: 0.0854 | Val METEOR: 0.4923\n",
      "14m 2s (- 8m 36s) (31 62.0%) | Train Loss: 0.084 | Val METEOR: 0.4666\n",
      "14m 29s (- 8m 8s) (32 64.0%) | Train Loss: 0.0826 | Val METEOR: 0.4969\n",
      "14m 56s (- 7m 41s) (33 66.0%) | Train Loss: 0.0813 | Val METEOR: 0.4812\n",
      "15m 23s (- 7m 14s) (34 68.0%) | Train Loss: 0.0805 | Val METEOR: 0.4834\n",
      "15m 50s (- 6m 47s) (35 70.0%) | Train Loss: 0.078 | Val METEOR: 0.4977\n",
      "16m 17s (- 6m 20s) (36 72.0%) | Train Loss: 0.0775 | Val METEOR: 0.4631\n",
      "16m 45s (- 5m 53s) (37 74.0%) | Train Loss: 0.0757 | Val METEOR: 0.4808\n",
      "17m 12s (- 5m 25s) (38 76.0%) | Train Loss: 0.0746 | Val METEOR: 0.4758\n",
      "17m 39s (- 4m 58s) (39 78.0%) | Train Loss: 0.0742 | Val METEOR: 0.4647\n",
      "18m 6s (- 4m 31s) (40 80.0%) | Train Loss: 0.071 | Val METEOR: 0.495\n",
      "18m 33s (- 4m 4s) (41 82.0%) | Train Loss: 0.0702 | Val METEOR: 0.4889\n",
      "19m 0s (- 3m 37s) (42 84.0%) | Train Loss: 0.0697 | Val METEOR: 0.4944\n",
      "19m 27s (- 3m 10s) (43 86.0%) | Train Loss: 0.0688 | Val METEOR: 0.4887\n",
      "19m 55s (- 2m 42s) (44 88.0%) | Train Loss: 0.0674 | Val METEOR: 0.4932\n",
      "20m 22s (- 2m 15s) (45 90.0%) | Train Loss: 0.0666 | Val METEOR: 0.4413\n",
      "No improvement in bleu score for 10 consecutive epochs. Stopping early.\n",
      "############################ decoder hidden size: 512 ############################\n",
      "encoder #parameters: 6545152\n",
      "decoder #parameters: 5353189\n",
      "0m 33s (- 27m 12s) (1 2.0%) | Train Loss: 0.4643 | Val METEOR: 0.3532\n",
      "1m 6s (- 26m 41s) (2 4.0%) | Train Loss: 0.2275 | Val METEOR: 0.3777\n",
      "1m 40s (- 26m 9s) (3 6.0%) | Train Loss: 0.1922 | Val METEOR: 0.3953\n",
      "2m 13s (- 25m 36s) (4 8.0%) | Train Loss: 0.1703 | Val METEOR: 0.4008\n",
      "2m 47s (- 25m 4s) (5 10.0%) | Train Loss: 0.156 | Val METEOR: 0.396\n",
      "3m 20s (- 24m 29s) (6 12.0%) | Train Loss: 0.1431 | Val METEOR: 0.3664\n",
      "3m 53s (- 23m 56s) (7 14.0%) | Train Loss: 0.1344 | Val METEOR: 0.4373\n",
      "4m 27s (- 23m 23s) (8 16.0%) | Train Loss: 0.1262 | Val METEOR: 0.4315\n",
      "5m 0s (- 22m 49s) (9 18.0%) | Train Loss: 0.12 | Val METEOR: 0.4481\n",
      "5m 34s (- 22m 17s) (10 20.0%) | Train Loss: 0.1136 | Val METEOR: 0.4272\n",
      "6m 7s (- 21m 43s) (11 22.0%) | Train Loss: 0.109 | Val METEOR: 0.432\n",
      "6m 41s (- 21m 9s) (12 24.0%) | Train Loss: 0.1034 | Val METEOR: 0.4518\n",
      "7m 14s (- 20m 36s) (13 26.0%) | Train Loss: 0.1001 | Val METEOR: 0.4318\n",
      "7m 47s (- 20m 3s) (14 28.0%) | Train Loss: 0.0973 | Val METEOR: 0.4322\n",
      "8m 21s (- 19m 29s) (15 30.0%) | Train Loss: 0.0945 | Val METEOR: 0.4469\n",
      "8m 54s (- 18m 56s) (16 32.0%) | Train Loss: 0.091 | Val METEOR: 0.461\n",
      "9m 28s (- 18m 23s) (17 34.0%) | Train Loss: 0.089 | Val METEOR: 0.4516\n",
      "10m 1s (- 17m 49s) (18 36.0%) | Train Loss: 0.0897 | Val METEOR: 0.4241\n",
      "10m 35s (- 17m 16s) (19 38.0%) | Train Loss: 0.0881 | Val METEOR: 0.4657\n",
      "11m 8s (- 16m 42s) (20 40.0%) | Train Loss: 0.0816 | Val METEOR: 0.472\n",
      "11m 42s (- 16m 9s) (21 42.0%) | Train Loss: 0.0789 | Val METEOR: 0.4826\n",
      "12m 15s (- 15m 36s) (22 44.0%) | Train Loss: 0.0786 | Val METEOR: 0.4674\n",
      "12m 48s (- 15m 2s) (23 46.0%) | Train Loss: 0.0765 | Val METEOR: 0.4605\n",
      "13m 22s (- 14m 29s) (24 48.0%) | Train Loss: 0.0756 | Val METEOR: 0.4627\n",
      "13m 55s (- 13m 55s) (25 50.0%) | Train Loss: 0.0741 | Val METEOR: 0.4767\n",
      "14m 29s (- 13m 22s) (26 52.0%) | Train Loss: 0.0713 | Val METEOR: 0.4776\n",
      "15m 2s (- 12m 48s) (27 54.0%) | Train Loss: 0.0709 | Val METEOR: 0.4622\n",
      "15m 35s (- 12m 15s) (28 56.0%) | Train Loss: 0.0693 | Val METEOR: 0.4789\n",
      "16m 9s (- 11m 41s) (29 58.0%) | Train Loss: 0.0682 | Val METEOR: 0.4728\n",
      "16m 42s (- 11m 8s) (30 60.0%) | Train Loss: 0.0672 | Val METEOR: 0.478\n",
      "17m 16s (- 10m 35s) (31 62.0%) | Train Loss: 0.0663 | Val METEOR: 0.481\n",
      "No improvement in bleu score for 10 consecutive epochs. Stopping early.\n",
      "############################ decoder hidden size: 1024 ############################\n",
      "encoder #parameters: 6545152\n",
      "decoder #parameters: 14373605\n",
      "0m 50s (- 41m 22s) (1 2.0%) | Train Loss: 0.3698 | Val METEOR: 0.2866\n",
      "1m 41s (- 40m 32s) (2 4.0%) | Train Loss: 0.212 | Val METEOR: 0.28\n",
      "2m 31s (- 39m 32s) (3 6.0%) | Train Loss: 0.9294 | Val METEOR: 0.0\n",
      "3m 21s (- 38m 33s) (4 8.0%) | Train Loss: 1.685 | Val METEOR: 0.0\n",
      "4m 11s (- 37m 39s) (5 10.0%) | Train Loss: 1.6741 | Val METEOR: 0.0\n",
      "5m 0s (- 36m 45s) (6 12.0%) | Train Loss: 1.6666 | Val METEOR: 0.0\n",
      "5m 50s (- 35m 53s) (7 14.0%) | Train Loss: 1.6684 | Val METEOR: 0.0\n",
      "6m 41s (- 35m 10s) (8 16.0%) | Train Loss: 1.6683 | Val METEOR: 0.0\n",
      "7m 33s (- 34m 26s) (9 18.0%) | Train Loss: 1.6651 | Val METEOR: 0.0\n",
      "8m 24s (- 33m 39s) (10 20.0%) | Train Loss: 1.6669 | Val METEOR: 0.0\n",
      "9m 15s (- 32m 49s) (11 22.0%) | Train Loss: 1.664 | Val METEOR: 0.0\n",
      "No improvement in bleu score for 10 consecutive epochs. Stopping early.\n",
      "############################ decoder hidden size: 2048 ############################\n",
      "encoder #parameters: 6545152\n",
      "decoder #parameters: 43424485\n",
      "1m 40s (- 81m 41s) (1 2.0%) | Train Loss: 0.3383 | Val METEOR: 0.3398\n",
      "3m 19s (- 79m 37s) (2 4.0%) | Train Loss: 0.2079 | Val METEOR: 0.2886\n",
      "4m 59s (- 78m 7s) (3 6.0%) | Train Loss: 0.1774 | Val METEOR: 0.3813\n",
      "6m 38s (- 76m 24s) (4 8.0%) | Train Loss: 0.1652 | Val METEOR: 0.3193\n",
      "8m 19s (- 74m 53s) (5 10.0%) | Train Loss: 0.1549 | Val METEOR: 0.3833\n",
      "9m 59s (- 73m 15s) (6 12.0%) | Train Loss: 0.1468 | Val METEOR: 0.3841\n",
      "11m 39s (- 71m 35s) (7 14.0%) | Train Loss: 0.1379 | Val METEOR: 0.377\n",
      "13m 18s (- 69m 54s) (8 16.0%) | Train Loss: 0.1325 | Val METEOR: 0.3621\n",
      "14m 58s (- 68m 11s) (9 18.0%) | Train Loss: 0.1275 | Val METEOR: 0.3977\n",
      "16m 38s (- 66m 33s) (10 20.0%) | Train Loss: 0.1263 | Val METEOR: 0.4053\n",
      "18m 46s (- 66m 34s) (11 22.0%) | Train Loss: 1.0863 | Val METEOR: 0.0\n",
      "20m 25s (- 64m 39s) (12 24.0%) | Train Loss: 1.7238 | Val METEOR: 0.0\n",
      "22m 8s (- 63m 2s) (13 26.0%) | Train Loss: 1.7008 | Val METEOR: 0.0\n",
      "23m 55s (- 61m 32s) (14 28.0%) | Train Loss: 1.6989 | Val METEOR: 0.0\n",
      "25m 42s (- 59m 58s) (15 30.0%) | Train Loss: 1.6991 | Val METEOR: 0.0\n",
      "27m 28s (- 58m 23s) (16 32.0%) | Train Loss: 1.6926 | Val METEOR: 0.0\n",
      "29m 15s (- 56m 47s) (17 34.0%) | Train Loss: 1.6842 | Val METEOR: 0.0\n",
      "31m 1s (- 55m 8s) (18 36.0%) | Train Loss: 1.6796 | Val METEOR: 0.0\n",
      "32m 43s (- 53m 23s) (19 38.0%) | Train Loss: 1.6808 | Val METEOR: 0.0\n",
      "34m 22s (- 51m 34s) (20 40.0%) | Train Loss: 1.6797 | Val METEOR: 0.0\n",
      "No improvement in bleu score for 10 consecutive epochs. Stopping early.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "language, dataloader = get_dataloader(file_path='./data_ptb-xl', batch_size=64, mode='train', device=device)\n",
    "_, val_dataloader = get_dataloader(file_path='./data_ptb-xl', batch_size=64, mode='val', device=device, _lang=language)\n",
    "    \n",
    "n_epochs=50\n",
    "encoder_hidden_size = 256\n",
    "decoder_hidden = [128, 256, 512, 1024, 2048]\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "for decoder_hidden_size in decoder_hidden:\n",
    "    encoder = NN(num_leads=12,\n",
    "                 hidden_size=encoder_hidden_size).to(device)\n",
    "    decoder = AttnDecoderRNN(hidden_size=decoder_hidden_size,\n",
    "                             encoder_hidden_size=encoder_hidden_size,\n",
    "                             output_size=language.n_words,\n",
    "                             max_len=language.max_len).to(device)\n",
    "    \n",
    "    print(f\"############################ decoder hidden size: {decoder_hidden_size} ############################\")\n",
    "    print(f\"encoder #parameters: {count_parameters(encoder)}\")\n",
    "    print(f\"decoder #parameters: {count_parameters(decoder)}\\n\")\n",
    "\n",
    "    train(dataloader, val_dataloader, encoder, decoder, criterion, language, n_epochs, size=decoder_hidden_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-23T07:21:11.558829Z",
     "start_time": "2024-05-23T05:38:57.054421Z"
    }
   },
   "id": "aed98c009ac2367d",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf1c8f19a4b4704e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "        Decoder  Encoder #parameters  Decoder #parameters  Test Loss  \\\n0   Decoder 128              6545152               996325     3.2047   \n1   Decoder 256              6545152              2219237     3.4594   \n2   Decoder 512              6545152              5353189     3.5877   \n3  Decoder 1024              6545152             14373605     2.7042   \n4  Decoder 2048              6545152             43424485     3.6610   \n\n   Rouge-1 (p)  Rouge-1 (r)  Rouge-1 (f1)  Rouge-2 (p)  Rouge-2 (r)  \\\n0        0.590        0.607         0.574        0.451        0.456   \n1        0.592        0.602         0.572        0.449        0.453   \n2        0.573        0.563         0.543        0.426        0.414   \n3        0.425        0.566         0.448        0.289        0.375   \n4        0.510        0.529         0.494        0.356        0.361   \n\n   Rouge-2 (f1)  Rouge-L (p)  Rouge-L (r)  Rouge-L (f1)  METEOR  \n0         0.433        0.587        0.604         0.570   0.504  \n1         0.431        0.588        0.597         0.568   0.501  \n2         0.400        0.568        0.559         0.538   0.476  \n3         0.297        0.424        0.564         0.447   0.284  \n4         0.338        0.507        0.526         0.491   0.399  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Decoder</th>\n      <th>Encoder #parameters</th>\n      <th>Decoder #parameters</th>\n      <th>Test Loss</th>\n      <th>Rouge-1 (p)</th>\n      <th>Rouge-1 (r)</th>\n      <th>Rouge-1 (f1)</th>\n      <th>Rouge-2 (p)</th>\n      <th>Rouge-2 (r)</th>\n      <th>Rouge-2 (f1)</th>\n      <th>Rouge-L (p)</th>\n      <th>Rouge-L (r)</th>\n      <th>Rouge-L (f1)</th>\n      <th>METEOR</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Decoder 128</td>\n      <td>6545152</td>\n      <td>996325</td>\n      <td>3.2047</td>\n      <td>0.590</td>\n      <td>0.607</td>\n      <td>0.574</td>\n      <td>0.451</td>\n      <td>0.456</td>\n      <td>0.433</td>\n      <td>0.587</td>\n      <td>0.604</td>\n      <td>0.570</td>\n      <td>0.504</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Decoder 256</td>\n      <td>6545152</td>\n      <td>2219237</td>\n      <td>3.4594</td>\n      <td>0.592</td>\n      <td>0.602</td>\n      <td>0.572</td>\n      <td>0.449</td>\n      <td>0.453</td>\n      <td>0.431</td>\n      <td>0.588</td>\n      <td>0.597</td>\n      <td>0.568</td>\n      <td>0.501</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Decoder 512</td>\n      <td>6545152</td>\n      <td>5353189</td>\n      <td>3.5877</td>\n      <td>0.573</td>\n      <td>0.563</td>\n      <td>0.543</td>\n      <td>0.426</td>\n      <td>0.414</td>\n      <td>0.400</td>\n      <td>0.568</td>\n      <td>0.559</td>\n      <td>0.538</td>\n      <td>0.476</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Decoder 1024</td>\n      <td>6545152</td>\n      <td>14373605</td>\n      <td>2.7042</td>\n      <td>0.425</td>\n      <td>0.566</td>\n      <td>0.448</td>\n      <td>0.289</td>\n      <td>0.375</td>\n      <td>0.297</td>\n      <td>0.424</td>\n      <td>0.564</td>\n      <td>0.447</td>\n      <td>0.284</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Decoder 2048</td>\n      <td>6545152</td>\n      <td>43424485</td>\n      <td>3.6610</td>\n      <td>0.510</td>\n      <td>0.529</td>\n      <td>0.494</td>\n      <td>0.356</td>\n      <td>0.361</td>\n      <td>0.338</td>\n      <td>0.507</td>\n      <td>0.526</td>\n      <td>0.491</td>\n      <td>0.399</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder_hidden_size = 256\n",
    "decoder_hidden = [128, 256, 512, 1024, 2048]\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "_, test_dataloader = get_dataloader(file_path='./data_ptb-xl', batch_size=64, mode='test', device=device, _lang=language)\n",
    "results = []\n",
    "\n",
    "for decoder_hidden_size in decoder_hidden:\n",
    "    encoder = NN(num_leads=12,\n",
    "                 hidden_size=encoder_hidden_size).to(device)\n",
    "    decoder = AttnDecoderRNN(hidden_size=decoder_hidden_size,\n",
    "                             encoder_hidden_size=encoder_hidden_size,\n",
    "                             output_size=language.n_words,\n",
    "                             max_len=language.max_len).to(device)\n",
    "    \n",
    "    encoder.load_state_dict(torch.load(f'./models/m04_EcgToText_ISIBrnoAIMT/models_with_different_hidden_size/varying_decoder/Encoder_{decoder_hidden_size}.pth'))\n",
    "    decoder.load_state_dict(torch.load(f'./models/m04_EcgToText_ISIBrnoAIMT/models_with_different_hidden_size/varying_decoder/Decoder_{decoder_hidden_size}.pth'))\n",
    "    \n",
    "    total_loss, f1, jaccard, rouge, meteor = validate_epoch(test_dataloader, encoder, decoder, criterion, language)\n",
    "\n",
    "    results.append({\n",
    "        \"Decoder\": f\"Decoder {decoder_hidden_size}\",\n",
    "        \"Encoder #parameters\": count_parameters(encoder),\n",
    "        \"Decoder #parameters\": count_parameters(decoder),\n",
    "        \"Test Loss\": round(total_loss, 4),\n",
    "        \"Rouge-1 (p)\": round(rouge[\"rouge-1\"][\"p\"], 3),\n",
    "        \"Rouge-1 (r)\": round(rouge[\"rouge-1\"][\"r\"], 3),\n",
    "        \"Rouge-1 (f1)\": round(rouge[\"rouge-1\"][\"f\"], 3),\n",
    "        \"Rouge-2 (p)\": round(rouge[\"rouge-2\"][\"p\"], 3),\n",
    "        \"Rouge-2 (r)\": round(rouge[\"rouge-2\"][\"r\"], 3),\n",
    "        \"Rouge-2 (f1)\": round(rouge[\"rouge-2\"][\"f\"], 3),\n",
    "        \"Rouge-L (p)\": round(rouge[\"rouge-l\"][\"p\"], 3),\n",
    "        \"Rouge-L (r)\": round(rouge[\"rouge-l\"][\"r\"], 3),\n",
    "        \"Rouge-L (f1)\": round(rouge[\"rouge-l\"][\"f\"], 3),\n",
    "        \"METEOR\": round(meteor, 3)\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-24T07:17:54.333409Z",
     "start_time": "2024-05-24T07:17:35.986255Z"
    }
   },
   "id": "c4009bd4ebcfb476",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-23T09:19:41.410083Z"
    }
   },
   "id": "251cceae8b06a8cb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
